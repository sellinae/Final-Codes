{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Go9C3uLL8Izc"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Import necessary modules and dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T08:04:40.945813Z",
     "iopub.status.busy": "2022-01-26T08:04:40.945133Z",
     "iopub.status.idle": "2022-01-26T08:04:44.130549Z",
     "shell.execute_reply": "2022-01-26T08:04:44.129979Z"
    },
    "id": "dzLKpmZICaWN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from IPython import display\n",
    "\n",
    "# Set the seed value for experiment reproducibility.\n",
    "seed = 43\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Export file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path = r'results/noise_factor_0.0001-0.002 OR shift_rnd data_fixed_100 (35-65_sec)/'\n",
    "if not os.path.exists(export_path):\n",
    "    os.makedirs(export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yR0EdgrLCaWR"
   },
   "source": [
    "## Import the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgvFq3uYiS5G"
   },
   "source": [
    "The dataset's audio clips are stored in two folders corresponding to each label: `HT` and `WT`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T08:04:49.312621Z",
     "iopub.status.busy": "2022-01-26T08:04:49.311989Z",
     "iopub.status.idle": "2022-01-26T08:04:49.314667Z",
     "shell.execute_reply": "2022-01-26T08:04:49.315026Z"
    },
    "id": "70IBxSKxA1N9"
   },
   "outputs": [],
   "source": [
    "commands = np.array(tf.io.gfile.listdir(str('split_data_aug_fixed')))\n",
    "commands = commands[commands != 'desktop.ini']\n",
    "print('Commands:', commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMvdU9SY8WXN"
   },
   "source": [
    "Extract the audio clips into a list called `filenames`, and shuffle it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data folder names:\n",
    "\n",
    "split_data - all the short files combined with silence to a single file per session <br>\n",
    "split_data_increased - all the short files combined with silence to a multiple files with a random length <br>\n",
    "split_data_const - the combined files from split_data are split to a constant length (with or without residue) <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T08:04:49.320460Z",
     "iopub.status.busy": "2022-01-26T08:04:49.319695Z",
     "iopub.status.idle": "2022-01-26T08:04:51.053400Z",
     "shell.execute_reply": "2022-01-26T08:04:51.052878Z"
    },
    "id": "hlX685l1wD9k"
   },
   "outputs": [],
   "source": [
    "filenames = tf.io.gfile.glob(str('split_data_aug_fixed') + '/*/*')\n",
    "filenames = tf.random.shuffle(filenames, seed=seed)\n",
    "num_samples = len(filenames)\n",
    "print('Number of total examples:', num_samples)\n",
    "\n",
    "ht_count = len(tf.io.gfile.listdir(str('split_data_aug_fixed' + '/' + commands[0])))\n",
    "print('Number of examples for HT label:', ht_count)\n",
    "wt_count = len(tf.io.gfile.listdir(str('split_data_aug_fixed' + '/' + commands[1])))\n",
    "print('Number of examples for WT label:', wt_count)\n",
    "\n",
    "print('Example file tensor:', filenames[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout\n",
    "\n",
    "with open(export_path + 'total_examples.txt', 'w') as f:\n",
    "    with redirect_stdout(f):\n",
    "        print(f\"\"\"\n",
    "        Number of total examples: {num_samples}\n",
    "        Number of examples for HT label: {ht_count}\n",
    "        Number of examples for WT label: {wt_count}\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vK3ymy23MCP"
   },
   "source": [
    "Split `filenames` into training, validation and test sets using a 70:15:15 ratio, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7*num_samples)\n",
    "val_size = int(0.85*num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T08:04:51.058614Z",
     "iopub.status.busy": "2022-01-26T08:04:51.057996Z",
     "iopub.status.idle": "2022-01-26T08:04:51.062220Z",
     "shell.execute_reply": "2022-01-26T08:04:51.061764Z"
    },
    "id": "Cv_wts-l3KgD"
   },
   "outputs": [],
   "source": [
    "train_files = filenames[:train_size]\n",
    "val_files = filenames[train_size:val_size]\n",
    "test_files = filenames[val_size:]\n",
    "\n",
    "print('Training set size', len(train_files))\n",
    "print('Validation set size', len(val_files))\n",
    "print('Test set size', len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout\n",
    "\n",
    "with open(export_path + 'train_val_test.txt', 'w') as f:\n",
    "    with redirect_stdout(f):\n",
    "        print(f\"\"\"\n",
    "        Training set size: {len(train_files)}\n",
    "        Validation set size: {len(val_files)}\n",
    "        Test set size: {len(test_files)}\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2Cj9FyvfweD"
   },
   "source": [
    "## Read the audio files and their labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1zjcWteOcBy"
   },
   "source": [
    "In this section you will preprocess the dataset, creating decoded tensors for the waveforms and the corresponding labels. Note that:\n",
    "\n",
    "- Each WAV file contains time-series data with a set number of samples per second.\n",
    "- Each sample represents the <a href=\"https://en.wikipedia.org/wiki/Amplitude\" class=\"external\">amplitude</a> of the audio signal at that specific time.\n",
    "- In a <a href=\"https://en.wikipedia.org/wiki/Audio_bit_depth\" class=\"external\">16-bit</a> system, like the WAV files in the mini Speech Commands dataset, the amplitude values range from -32,768 to 32,767.\n",
    "- The <a href=\"https://en.wikipedia.org/wiki/Sampling_(signal_processing)#Audio_sampling\" class=\"external\">sample rate</a> for this dataset is 16kHz.\n",
    "\n",
    "The shape of the tensor returned by `tf.audio.decode_wav` is `[samples, channels]`, where `channels` is `1` for mono or `2` for stereo. The mini Speech Commands dataset only contains mono recordings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T08:04:51.067167Z",
     "iopub.status.busy": "2022-01-26T08:04:51.066572Z",
     "iopub.status.idle": "2022-01-26T08:04:51.071744Z",
     "shell.execute_reply": "2022-01-26T08:04:51.072134Z"
    },
    "id": "d16bb8416f90"
   },
   "outputs": [],
   "source": [
    "# test_file = tf.io.read_file('split_data\\\\WT\\\\syllable6.wav')\n",
    "# test_audio, _ = tf.audio.decode_wav(contents=test_file)\n",
    "# test_audio.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6bb8defd2ef"
   },
   "source": [
    "Now, let's define a function that preprocesses the dataset's raw WAV audio files into audio tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T08:04:51.076854Z",
     "iopub.status.busy": "2022-01-26T08:04:51.076218Z",
     "iopub.status.idle": "2022-01-26T08:04:51.077976Z",
     "shell.execute_reply": "2022-01-26T08:04:51.078334Z"
    },
    "id": "9PjJ2iXYwftD"
   },
   "outputs": [],
   "source": [
    "def decode_audio(audio_binary):\n",
    "  # Decode WAV-encoded audio files to `float32` tensors, normalized\n",
    "  # to the [-1.0, 1.0] range. Return `float32` audio and a sample rate.\n",
    "  audio, _ = tf.audio.decode_wav(contents=audio_binary)\n",
    "  # Since all the data is single channel (mono), drop the `channels`\n",
    "  # axis from the array.\n",
    "  return tf.squeeze(audio, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPQseZElOjVN"
   },
   "source": [
    "Define a function that creates labels using the parent directories for each file:\n",
    "\n",
    "- Split the file paths into `tf.RaggedTensor`s (tensors with ragged dimensionsâ€”with slices that may have different lengths)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T08:04:51.082722Z",
     "iopub.status.busy": "2022-01-26T08:04:51.082140Z",
     "iopub.status.idle": "2022-01-26T08:04:51.083771Z",
     "shell.execute_reply": "2022-01-26T08:04:51.084169Z"
    },
    "id": "8VTtX1nr3YT-"
   },
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "  parts = tf.strings.split(\n",
    "      input=file_path,\n",
    "      sep=os.path.sep)\n",
    "  # Note: You'll use indexing here instead of tuple unpacking to enable this\n",
    "  # to work in a TensorFlow graph.\n",
    "  return parts[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8Y9w_5MOsr-"
   },
   "source": [
    "Define another helper functionâ€”`get_waveform_and_label`â€”that puts it all together:\n",
    "\n",
    "- The input is the WAV audio filename.\n",
    "- The output is a tuple containing the audio and label tensors ready for supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T08:04:51.088167Z",
     "iopub.status.busy": "2022-01-26T08:04:51.087551Z",
     "iopub.status.idle": "2022-01-26T08:04:51.090110Z",
     "shell.execute_reply": "2022-01-26T08:04:51.089641Z"
    },
    "id": "WdgUD5T93NyT"
   },
   "outputs": [],
   "source": [
    "def get_waveform_and_label(file_path):\n",
    "  label = get_label(file_path)\n",
    "  audio_binary = tf.io.read_file(file_path)\n",
    "  waveform = decode_audio(audio_binary)\n",
    "  return waveform, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvN8W_dDjYjc"
   },
   "source": [
    "Build the training set to extract the audio-label pairs:\n",
    "\n",
    "- Create a `tf.data.Dataset` with `Dataset.from_tensor_slices` and `Dataset.map`, using `get_waveform_and_label` defined earlier.\n",
    "\n",
    "You'll build the validation and test sets using a similar procedure later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T08:04:51.097251Z",
     "iopub.status.busy": "2022-01-26T08:04:51.096180Z",
     "iopub.status.idle": "2022-01-26T08:04:51.230089Z",
     "shell.execute_reply": "2022-01-26T08:04:51.230527Z"
    },
    "id": "0SQl8yXl3kNP"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "files_ds = tf.data.Dataset.from_tensor_slices(train_files)\n",
    "\n",
    "waveform_ds = files_ds.map(\n",
    "    map_func=get_waveform_and_label,\n",
    "    num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voxGEwvuh2L7"
   },
   "source": [
    "Let's plot a few audio waveforms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T08:04:51.357783Z",
     "iopub.status.busy": "2022-01-26T08:04:51.357061Z",
     "iopub.status.idle": "2022-01-26T08:04:52.086735Z",
     "shell.execute_reply": "2022-01-26T08:04:52.087151Z"
    },
    "id": "8yuX6Nqzf6wT"
   },
   "outputs": [],
   "source": [
    "# rows = 1\n",
    "# cols = 1\n",
    "# n = rows * cols\n",
    "# fig, axes = plt.subplots(rows, cols, figsize=(10, 12))\n",
    "\n",
    "# for i, (audio, label) in enumerate(waveform_ds.take(n)):\n",
    "#   r = i // cols\n",
    "#   c = i % cols\n",
    "#   ax = axes[r][c]\n",
    "#   ax.plot(audio.numpy())\n",
    "#   ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n",
    "#   label = label.numpy().decode('utf-8')\n",
    "#   ax.set_title(label)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWXPphxm0B4m"
   },
   "source": [
    "## Convert waveforms to spectrograms\n",
    "\n",
    "The waveforms in the dataset are represented in the time domain. Next, you'll transform the waveforms from the time-domain signals into the time-frequency-domain signals by computing the <a href=\"https://en.wikipedia.org/wiki/Short-time_Fourier_transform\" class=\"external\">short-time Fourier transform (STFT)</a> to convert the waveforms to as <a href=\"https://en.wikipedia.org/wiki/Spectrogram\" clas=\"external\">spectrograms</a>, which show frequency changes over time and can be represented as 2D images. You will feed the spectrogram images into your neural network to train the model.\n",
    "\n",
    "A Fourier transform (`tf.signal.fft`) converts a signal to its component frequencies, but loses all time information. In comparison, STFT (`tf.signal.stft`) splits the signal into windows of time and runs a Fourier transform on each window, preserving some time information, and returning a 2D tensor that you can run standard convolutions on.\n",
    "\n",
    "Create a utility function for converting waveforms to spectrograms:\n",
    "\n",
    "- The waveforms need to be of the same length, so that when you convert them to spectrograms, the results have similar dimensions. This can be done by simply zero-padding the audio clips that are shorter than one second (using `tf.zeros`).\n",
    "- When calling `tf.signal.stft`, choose the `frame_length` and `frame_step` parameters such that the generated spectrogram \"image\" is almost square. For more information on the STFT parameters choice, refer to <a href=\"https://www.coursera.org/lecture/audio-signal-processing/stft-2-tjEQe\" class=\"external\">this Coursera video</a> on audio signal processing and STFT.\n",
    "- The STFT produces an array of complex numbers representing magnitude and phase. However, in this tutorial you'll only use the magnitude, which you can derive by applying `tf.abs` on the output of `tf.signal.stft`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T08:04:52.093916Z",
     "iopub.status.busy": "2022-01-26T08:04:52.093060Z",
     "iopub.status.idle": "2022-01-26T08:04:52.095594Z",
     "shell.execute_reply": "2022-01-26T08:04:52.095070Z"
    },
    "id": "_4CK75DHz_OR"
   },
   "outputs": [],
   "source": [
    "def get_spectrogram(waveform):\n",
    "  # Zero-padding for an audio waveform with less than 16,000 samples.\n",
    "  input_len = 16000\n",
    "  waveform = waveform[:input_len]\n",
    "  zero_padding = tf.zeros(\n",
    "      [16000] - tf.shape(waveform),\n",
    "      dtype=tf.float32)\n",
    "  # Cast the waveform tensors' dtype to float32.\n",
    "  waveform = tf.cast(waveform, dtype=tf.float32)\n",
    "  # Concatenate the waveform with `zero_padding`, which ensures all audio\n",
    "  # clips are of the same length.\n",
    "  equal_length = tf.concat([waveform, zero_padding], 0)\n",
    "  # Convert the waveform to a spectrogram via a STFT.\n",
    "  spectrogram = tf.signal.stft(\n",
    "      equal_length, frame_length=255, frame_step=128)\n",
    "  # Obtain the magnitude of the STFT.\n",
    "  spectrogram = tf.abs(spectrogram)\n",
    "  # Add a `channels` dimension, so that the spectrogram can be used\n",
    "  # as image-like input data with convolution layers (which expect\n",
    "  # shape (`batch_size`, `height`, `width`, `channels`).\n",
    "  spectrogram = spectrogram[..., tf.newaxis]\n",
    "  return spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rdPiPYJphs2"
   },
   "source": [
    "Next, start exploring the data. Print the shapes of one example's tensorized waveform and the corresponding spectrogram, and play the original audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T08:04:52.101770Z",
     "iopub.status.busy": "2022-01-26T08:04:52.100842Z",
     "iopub.status.idle": "2022-01-26T08:04:52.415977Z",
     "shell.execute_reply": "2022-01-26T08:04:52.415446Z"
    },
    "id": "4Mu6Y7Yz3C-V"
   },
   "outputs": [],
   "source": [
    "for waveform, label in waveform_ds.take(1):\n",
    "  label = label.numpy().decode('utf-8')\n",
    "  spectrogram = get_spectrogram(waveform)\n",
    "\n",
    "print('Label:', label)\n",
    "print('Waveform shape:', waveform.shape)\n",
    "print('Spectrogram shape:', spectrogram.shape)\n",
    "print('Audio playback')\n",
    "display.display(display.Audio(waveform, rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnSuqyxJ1isF"
   },
   "source": [
    "Now, define a function for displaying a spectrogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T08:04:52.422463Z",
     "iopub.status.busy": "2022-01-26T08:04:52.421877Z",
     "iopub.status.idle": "2022-01-26T08:04:52.423570Z",
     "shell.execute_reply": "2022-01-26T08:04:52.423981Z"
    },
    "id": "e62jzb36-Jog"
   },
   "outputs": [],
   "source": [
    "def plot_spectrogram(spectrogram, ax):\n",
    "  if len(spectrogram.shape) > 2:\n",
    "    assert len(spectrogram.shape) == 3\n",
    "    spectrogram = np.squeeze(spectrogram, axis=-1)\n",
    "  # Convert the frequencies to log scale and transpose, so that the time is\n",
    "  # represented on the x-axis (columns).\n",
    "  # Add an epsilon to avoid taking a log of zero.\n",
    "  log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n",
    "  height = log_spec.shape[0]\n",
    "  width = log_spec.shape[1]\n",
    "  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
    "  Y = range(height)\n",
    "  ax.pcolormesh(X, Y, log_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baa5c91e8603"
   },
   "source": [
    "Plot the example's waveform over time and the corresponding spectrogram (frequencies over time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T08:04:52.458644Z",
     "iopub.status.busy": "2022-01-26T08:04:52.457933Z",
     "iopub.status.idle": "2022-01-26T08:04:52.694674Z",
     "shell.execute_reply": "2022-01-26T08:04:52.695148Z"
    },
    "id": "d2_CikgY1tjv"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
    "timescale = np.arange(waveform.shape[0])\n",
    "axes[0].plot(timescale, waveform.numpy())\n",
    "axes[0].set_title('Waveform')\n",
    "axes[0].set_xlim([0, 16000])\n",
    "\n",
    "plot_spectrogram(spectrogram.numpy(), axes[1])\n",
    "axes[1].set_title('Spectrogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyYXjW07jCHA"
   },
   "source": [
    "Now, define a function that transforms the waveform dataset into spectrograms and their corresponding labels as integer IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T08:04:52.700279Z",
     "iopub.status.busy": "2022-01-26T08:04:52.699678Z",
     "iopub.status.idle": "2022-01-26T08:04:52.701942Z",
     "shell.execute_reply": "2022-01-26T08:04:52.701499Z"
    },
    "id": "43IS2IouEV40"
   },
   "outputs": [],
   "source": [
    "def get_spectrogram_and_label_id(audio, label):\n",
    "  spectrogram = get_spectrogram(audio)\n",
    "  label_id = tf.argmax(label == commands)\n",
    "  return spectrogram, label_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cf5d5b033a45"
   },
   "source": [
    "Map `get_spectrogram_and_label_id` across the dataset's elements with `Dataset.map`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T08:04:52.706832Z",
     "iopub.status.busy": "2022-01-26T08:04:52.706072Z",
     "iopub.status.idle": "2022-01-26T08:04:52.870120Z",
     "shell.execute_reply": "2022-01-26T08:04:52.869507Z"
    },
    "id": "yEVb_oK0oBLQ"
   },
   "outputs": [],
   "source": [
    "spectrogram_ds = waveform_ds.map(\n",
    "  map_func=get_spectrogram_and_label_id,\n",
    "  num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gQpAAgMnyDi"
   },
   "source": [
    "Examine the spectrograms for different examples of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T08:04:52.913705Z",
     "iopub.status.busy": "2022-01-26T08:04:52.900418Z",
     "iopub.status.idle": "2022-01-26T08:04:53.608844Z",
     "shell.execute_reply": "2022-01-26T08:04:53.609299Z"
    },
    "id": "QUbHfTuon4iF"
   },
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = 3\n",
    "n = rows*cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(10, 10))\n",
    "\n",
    "for i, (spectrogram, label_id) in enumerate(spectrogram_ds.take(n)):\n",
    "  r = i // cols\n",
    "  c = i % cols\n",
    "  ax = axes[r][c]\n",
    "  plot_spectrogram(spectrogram.numpy(), ax)\n",
    "  ax.set_title(commands[label_id.numpy()])\n",
    "  ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5KdY8IF8rkt"
   },
   "source": [
    "## Build and train the model\n",
    "\n",
    "Repeat the training set preprocessing on the validation and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T08:04:53.615537Z",
     "iopub.status.busy": "2022-01-26T08:04:53.614754Z",
     "iopub.status.idle": "2022-01-26T08:04:53.617519Z",
     "shell.execute_reply": "2022-01-26T08:04:53.617014Z"
    },
    "id": "10UI32QH_45b"
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(files):\n",
    "  files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "  output_ds = files_ds.map(\n",
    "      map_func=get_waveform_and_label,\n",
    "      num_parallel_calls=AUTOTUNE)\n",
    "  output_ds = output_ds.map(\n",
    "      map_func=get_spectrogram_and_label_id,\n",
    "      num_parallel_calls=AUTOTUNE)\n",
    "  return output_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T08:04:53.623176Z",
     "iopub.status.busy": "2022-01-26T08:04:53.622513Z",
     "iopub.status.idle": "2022-01-26T08:04:53.863062Z",
     "shell.execute_reply": "2022-01-26T08:04:53.862454Z"
    },
    "id": "HNv4xwYkB2P6"
   },
   "outputs": [],
   "source": [
    "train_ds = spectrogram_ds\n",
    "val_ds = preprocess_dataset(val_files)\n",
    "test_ds = preprocess_dataset(test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "assnWo6SB3lR"
   },
   "source": [
    "Batch the training and validation sets for model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T08:04:53.868951Z",
     "iopub.status.busy": "2022-01-26T08:04:53.868150Z",
     "iopub.status.idle": "2022-01-26T08:04:53.872013Z",
     "shell.execute_reply": "2022-01-26T08:04:53.871372Z"
    },
    "id": "UgY9WYzn61EX"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "val_ds = val_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GS1uIh6F_TN9"
   },
   "source": [
    "Add `Dataset.cache` and `Dataset.prefetch` operations to reduce read latency while training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T08:04:53.877485Z",
     "iopub.status.busy": "2022-01-26T08:04:53.876594Z",
     "iopub.status.idle": "2022-01-26T08:04:53.879897Z",
     "shell.execute_reply": "2022-01-26T08:04:53.880309Z"
    },
    "id": "fdZ6M-F5_QzY"
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwHkKCQQb5oW"
   },
   "source": [
    "For the model, you'll use a simple convolutional neural network (CNN), since you have transformed the audio files into spectrogram images.\n",
    "\n",
    "Your `tf.keras.Sequential` model will use the following Keras preprocessing layers:\n",
    "\n",
    "- `tf.keras.layers.Resizing`: to downsample the input to enable the model to train faster.\n",
    "- `tf.keras.layers.Normalization`: to normalize each pixel in the image based on its mean and standard deviation.\n",
    "\n",
    "For the `Normalization` layer, its `adapt` method would first need to be called on the training data in order to compute aggregate statistics (that is, the mean and the standard deviation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for spectrogram, _ in spectrogram_ds.take(1):\n",
    "  input_shape = spectrogram.shape\n",
    "print('Input shape:', input_shape)\n",
    "num_labels = len(commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler no. 1\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# This is a sample of a scheduler I used in the past\n",
    "def lr_scheduler(epoch, lr):\n",
    "    decay_rate = 0.85\n",
    "    decay_step = 5\n",
    "    if epoch % decay_step == 0 and epoch:\n",
    "        return lr * pow(decay_rate, np.floor(epoch / decay_step))\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler no. 2\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "  if epoch < 15:\n",
    "    return lr\n",
    "  else:\n",
    "    return lr * tf.math.exp(-0.07)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler no. 3\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# This is a sample of a scheduler I used in the past\n",
    "def new_lr_scheduler(epoch, lr):\n",
    "    decay_rate = 0.5\n",
    "    decay_step = 1\n",
    "    if epoch % decay_step == 0 and epoch:\n",
    "        return lr * tf.math.exp(-(decay_rate / epoch))\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = ht_count + wt_count\n",
    "weight_for_0 = (1 / ht_count)*(total_count)/2.0\n",
    "weight_for_1 = (1 / wt_count)*(total_count)/2.0\n",
    "\n",
    "print('Weight for class 0 (HT): {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1 (WT): {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 2\n",
    "input_shape = (124, 129)\n",
    "\n",
    "model = models.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.LSTM(4096, return_sequences=True),\n",
    "        layers.TimeDistributed(layers.Dense(32, activation=\"relu\")),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.45),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.45),\n",
    "        layers.Dense(num_labels, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de52e5afa2f3"
   },
   "source": [
    "Configure the Keras model with the Adam optimizer and the cross-entropy loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Plot Model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import plot_model\n",
    "# plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T08:05:03.272380Z",
     "iopub.status.busy": "2022-01-26T08:05:03.271757Z",
     "iopub.status.idle": "2022-01-26T08:05:03.281300Z",
     "shell.execute_reply": "2022-01-26T08:05:03.281634Z"
    },
    "id": "wFjj7-EmsTD-"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f42b9e3a4705"
   },
   "source": [
    "Train the model over 200 epochs for demonstration purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T08:05:03.286913Z",
     "iopub.status.busy": "2022-01-26T08:05:03.286335Z",
     "iopub.status.idle": "2022-01-26T08:05:14.066442Z",
     "shell.execute_reply": "2022-01-26T08:05:14.066885Z"
    },
    "id": "ttioPJVMcGtq"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "CLASS_WEIGHT = {0: weight_for_0, 1: weight_for_1}\n",
    "CALLBACKS = [LearningRateScheduler(scheduler, verbose=1), tf.keras.callbacks.EarlyStopping(monitor='val_loss', verbose=1, patience=60)]\n",
    "#CALLBACKS = [LearningRateScheduler(lr_scheduler, verbose=1), tf.keras.callbacks.EarlyStopping(monitor='val_loss', verbose=1, patience=40)]\n",
    "#CALLBACKS = [LearningRateScheduler(new_lr_scheduler, verbose=1), tf.keras.callbacks.EarlyStopping(monitor='val_loss', verbose=1, patience=40)]\n",
    "#CALLBACKS = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', verbose=1, patience=40)]\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=CALLBACKS,\n",
    "    class_weight=CLASS_WEIGHT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjpCDeQ4mUfS"
   },
   "source": [
    "Let's plot the training and validation loss curves to check how your model has improved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "plt.style.use('default')\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.figure(facecolor='white')\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower right')\n",
    "plt.savefig(export_path + 'accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# summarize history for loss\n",
    "plt.figure(facecolor='white')\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.savefig(export_path + 'loss.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZTt3kO3mfm4"
   },
   "source": [
    "## Evaluate the model performance\n",
    "\n",
    "Run the model on the test set and check the model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T08:05:14.211012Z",
     "iopub.status.busy": "2022-01-26T08:05:14.209773Z",
     "iopub.status.idle": "2022-01-26T08:05:14.830928Z",
     "shell.execute_reply": "2022-01-26T08:05:14.830111Z"
    },
    "id": "biU2MwzyAo8o"
   },
   "outputs": [],
   "source": [
    "test_audio = []\n",
    "test_labels = []\n",
    "\n",
    "for audio, label in test_ds:\n",
    "  test_audio.append(audio.numpy())\n",
    "  test_labels.append(label.numpy())\n",
    "\n",
    "test_audio = np.array(test_audio)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T08:05:14.893616Z",
     "iopub.status.busy": "2022-01-26T08:05:14.892785Z",
     "iopub.status.idle": "2022-01-26T08:05:15.121509Z",
     "shell.execute_reply": "2022-01-26T08:05:15.120984Z"
    },
    "id": "ktUanr9mRZky"
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(test_audio), axis=1)\n",
    "y_true = test_labels\n",
    "\n",
    "test_acc = sum(y_pred == y_true) / len(y_true)\n",
    "print(f'Test set accuracy: {test_acc:.0%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout\n",
    "\n",
    "with open(export_path + 'test_accuracy.txt', 'w') as f:\n",
    "    with redirect_stdout(f):\n",
    "        print(f\"\"\"\n",
    "        Test set accuracy: {test_acc:.0%}\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "en9Znt1NOabH"
   },
   "source": [
    "### Display a confusion matrix\n",
    "\n",
    "Use a <a href=\"https://developers.google.com/machine-learning/glossary#confusion-matrix\" class=\"external\">confusion matrix</a> to check how well the model did classifying each of the commands in the test set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Get the confusion matrix\n",
    "cm  = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "HT_ROW_TOTAL = cm.flatten()[0] + cm.flatten()[1]\n",
    "WT_ROM_TOTAL = cm.flatten()[2] + cm.flatten()[3]\n",
    "\n",
    "row_percentages = [(cm.flatten()[0]/HT_ROW_TOTAL), cm.flatten()[1]/HT_ROW_TOTAL, cm.flatten()[2]/WT_ROM_TOTAL, cm.flatten()[3]/WT_ROM_TOTAL]\n",
    "row_percentages = [\"{0:.2%}\".format(value) for value in row_percentages]\n",
    "group_counts = [\"({0:0.0f})\".format(value) for value in cm.flatten()]\n",
    "\n",
    "labels = [f\"{v1} {v2}\" for v1, v2 in zip(row_percentages, group_counts)]\n",
    "\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "plt.figure(figsize=(10, 8), facecolor='white')\n",
    "sns.set(font_scale=1.6)\n",
    "ax = sns.heatmap(cm, annot=labels, fmt='', xticklabels=commands, yticklabels=commands)\n",
    "ax.set_title('Confusion Matrix:\\n\\n')\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ')\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.savefig(export_path + 'confusion_matrix.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prescision and recall calculation\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = ['HT', 'WT']\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout\n",
    "\n",
    "with open(export_path + 'classification_report.txt', 'w') as f:\n",
    "    with redirect_stdout(f):\n",
    "        print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQGi_mzPcLvl"
   },
   "source": [
    "## Run inference on an audio file\n",
    "\n",
    "Finally, verify the model's prediction output using an input audio file of someone saying \"no\". How well does your model perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "sample_file = filenames[random.randint(0,100)].numpy().decode(\"utf-8\")\n",
    "sample_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T08:05:15.499851Z",
     "iopub.status.busy": "2022-01-26T08:05:15.499256Z",
     "iopub.status.idle": "2022-01-26T08:05:15.816500Z",
     "shell.execute_reply": "2022-01-26T08:05:15.815899Z"
    },
    "id": "zRxauKMdhofU"
   },
   "outputs": [],
   "source": [
    "sample_ds = preprocess_dataset([str(sample_file)])\n",
    "\n",
    "for spectrogram, label in sample_ds.batch(1):\n",
    "  prediction = model(spectrogram)\n",
    "  plt.figure(facecolor='white')\n",
    "  plt.bar(commands, tf.nn.softmax(prediction[0]))\n",
    "  plt.title(f'Predictions for \"{commands[label[0]]}\"')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgWICqdqQNaQ"
   },
   "source": [
    "As the output suggests, your model should have recognized the audio command as \"no\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3jF933m9z1J"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "This tutorial demonstrated how to carry out simple audio classification/automatic speech recognition using a convolutional neural network with TensorFlow and Python. To learn more, consider the following resources:\n",
    "\n",
    "- The [Sound classification with YAMNet](https://www.tensorflow.org/hub/tutorials/yamnet) tutorial shows how to use transfer learning for audio classification.\n",
    "- The notebooks from <a href=\"https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/overview\" class=\"external\">Kaggle's TensorFlow speech recognition challenge</a>.\n",
    "- The \n",
    "<a href=\"https://codelabs.developers.google.com/codelabs/tensorflowjs-audio-codelab/index.html#0\" class=\"external\">TensorFlow.js - Audio recognition using transfer learning codelab</a> teaches how to build your own interactive web app for audio classification.\n",
    "- <a href=\"https://arxiv.org/abs/1709.04396\" class=\"external\">A tutorial on deep learning for music information retrieval</a> (Choi et al., 2017) on arXiv.\n",
    "- TensorFlow also has additional support for [audio data preparation and augmentation](https://www.tensorflow.org/io/tutorials/audio) to help with your own audio-based projects.\n",
    "- Consider using the <a href=\"https://librosa.org/\" class=\"external\">librosa</a> libraryâ€”a Python package for music and audio analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "\n",
    "def make_noise():\n",
    "  winsound.Beep(440, 180)\n",
    "  winsound.Beep(550, 180)\n",
    "  winsound.Beep(440, 180)\n",
    "  winsound.Beep(550, 180)\n",
    "\n",
    "\n",
    "make_noise()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "simple_audio.ipynb",
   "toc_visible": true
  },
  "interpreter": {
   "hash": "d905744c28536cd11da8c758e96462fdb93ef6b8e0526913cfd8b007abe5e1a6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
